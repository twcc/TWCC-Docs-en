---
title: 服務概觀 | en
tags: Guide, TWNIA2, EN
GA: UA-155999456-1
---

{%hackmd @docsharedstyle/default %}
{%hackmd @docsharedstyle/twccheader-en %}

# Service overview

## Taiwania2 - AI supercomputer

Taiwania2 is an AI supercomputer mainframe, using 2,016  NVIDIA® Tesla® V100 GPU, with 9 PFLOPS(9 peta floating-point operations per second) performance. In 2018, Taiwania2 [ranked 20 of the TOP500 list](https://www.top500.org/system/179590/), and [ranked 10 of the Green500 list](https://www.top500.org/lists/green500/2018/11/).


![](https://twcc-wordpress-file.cos.twcc.ai/wp-content/uploads/2019/09/19130553/1-2.png)


Taiwania2 provides various services, including  [Interactive Container](https://man.twcc.ai/@twccdocs/doc-ccs-main-en), [Scheduled Container](https://man.twcc.ai/@twccdocs/HyMqnHupV?type=view),  [HPC Job](https://man.twcc.ai/@twccdocs/HyVzTSOpN?type=view) and Taiwania2 (HPC CLI) (This service).


## Taiwania2 (HPC CLI) - Computing service


Taiwania2 (HPC CLI) computing service consists of **compute node**, **Hyper File System node** and **login and data transmission node**. Taiwania2 (HPC CLI) uses **Slurm** which supports MPI and is fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters.
- Allocating exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time
- Providing a framework for starting, executing, and monitoring work on the set of allocated nodes
- Arbitrating contention for resources by managing a queue of pending work

Also, using with **Singularity** container system to run the container, users can use Docker image provided by Taiwania2 to perform AI computing, and can quickly copy and rewrite the container image. It makes work environment maintenance cost low, and performs excellent massively parallel computing.

Benefits of Taiwania2 (HPC CLI):

- **Multi-node GPU allocation makes HPC distributed, parallel computing a reality**
    Using Slurm resources allocation manager to operate the powerful HPC can make multi-node parallel computing a reality. Evenly distributing high workload improves processing efficiency.
- **High bandwidth networks connect nodes for rapid data transmission**
    Using 100 Gbs high-speed networks to connect GPU clusters produces extremely high throughput and extremely low latency, and it solves the bottleneck of traditional technologies. When it comes to the efficiency of huge-volume data transmission, we never compromise.
- **GPU Direct and RDMA architecture for extreme accelerating**
    Integration of NVLink and InfiniBand makes RDMA (Remote Direct Memory Access) technology significantly improve the efficiency of multi-node big data transmission and improve the computing efficiency.
