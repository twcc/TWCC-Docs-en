---
tags: HowTo, conda, tutorial, TWCC, EN
title: HowTo：使用 Conda 管理套件與執行 Job | en
GA: UA-155999456-1
---

{%hackmd @docsharedstyle/default %}
{%hackmd @docsharedstyle/twccheader-en %}


# HowTo: Use Conda to manage packages and run jobs

:::success
<i class="fa fa-star" aria-hidden="true"></i> **Scenario: How to install packages in TAIWANIA 2 (HPC CLI)? How to switch Python environment?**

*Do you have the same problem? Let us show you how to integrate TWCC services to solve the problems you are facing with!*
:::

In this document, we will introduce the package management tool in TWCC TAIWANIA 2 (HPC CLI) - [Miniconda](https://docs.conda.io/en/latest/miniconda.html), and teach how to use Conda to create a virtual environment, install packages, and submit jobs.

## Introduction to Minoconda

Miniconda is a Python environment management system that includes a variety of Python packages and uses Conda as a package manager.

With Miniconda, use simple Conda commands to install  packages and switch to the specified virtual environment to use different versions of Python to solve the compatibility issues of multiple versions.



## Update planning

Due to the package manager Conda [fix bugs frequently](https://docs.conda.io/projects/conda/en/latest/release-notes.html), recent updates have greatly improved the user experience.

To allow TWCC users use the new version tool as soon as possible, after tests, TWCC will continue to update Miniconda (minimal installer for conda), and removed Anaconda.

:::info
<i class="fa fa-paperclip fa-20" aria-hidden="true"></i> **Note:** You may install Anaconda under your own `/home/$USER`  or `/work/$USER` directory.
:::

## Redesign modulefile

Generally, use `conda init` to set the environment in Aanaconda / Miniconda. At this time, Conda will change the user's `~/.bashrc` and add the following content.


```bash
# >>> conda initialize >>>
...
...
# <<< conda initialize <<<
```
For those who are new to Linux or Conda, it is not a friendly design. When Conda is no longer used, you may forget to remove the Conda parameter in `~/.bashrc`, and it might cause other operations be affected.

Therefore, TWCC rewrite the new version of the modulefile. When using `module load miniconda2` or `module load miniconda3`, the above parameters will be automatically activated; when `module purge`, these environment variables will be automatically removed. In this way, it can prevent Conda from changing the `~/.bashrc`, and use Conda properly, finally, bring user a simple environment.



## How to use Miniconda ? 

- Use `module load miniconda2` / `module load miniconda3` command to load the conda environment.
- Use `module purge` command to uninstall the conda environment.
- Due to [the above reasons](#Redesign-modulefile), we recommended you **remove** the conda initialized contents before use (the remove command `conda init --reverse`).


:::info
<i class="fa fa-paperclip fa-20" aria-hidden="true"></i> **Note:**
- miniconda2: Use Python 2 by default
- miniconda3: Use Python 3 by default
:::

Please refer to the following operation example.

## Conda operation example

Here is an example of using Conda to create a virtual environment, install TensorFlow that supports GPU computing, and then use Slurm Workload Manager to write job scripts to request resources, queue schedules, and submit jobs.

:::info
<i class="fa fa-paperclip fa-20" aria-hidden="true"></i> **Note:** References
- [<ins>Anaconda: TensorFlow</ins>](https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/)
- [<ins>Conda: Managing environments</ins>](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html)
:::


### Step 1. Load Conda and create a virtual environment
```bash
# Clear the module to make sure that the environment is clean
$ module purge

# Load Conda, and take miniconda3 with python3 as an example
$ module load miniconda3

# Because the miniconda module designed by TWCC will automatically initialize environment variables, there is no need to run conda init.
# We recommended you remove the conda initialized contents 
$ conda init --reverse

# Create a Conda virtual environment (In example, named mytf_gpu), and install tensorflow-gpu wrapped by Anaconda.
# The environment already contains CUDA and cuDNN, you do not need to install them by yourself.
$ conda create --name mytf_gpu tensorflow-gpu

# Enter the virtual environment
$ conda activate mytf_gpu
```

:::info
<i class="fa fa-paperclip fa-20" aria-hidden="true"></i> **Note:** 
tensorflow-gpu version 2.2 is currently installed by default
- You can choose to install version 1 or 2, for example, use 
 `conda create --name mytf_gpu tensorflow-gpu=1.15.0` for 1.15.0 version of tensorflow-gpu
 - For other versions, refer to [<ins>version list</ins>](https://anaconda.org/anaconda/tensorflow-gpu/files).
:::

#### Deactivate and delete the virtual environment
```bash
# deactivate virtual environment named mytf_gpu
$ conda deactivate

# delete virtual environment named mytf_gpu 
$ conda remove --name mytf_gpu --all
```

#### Unload Conda
```bash
# Uninstall all loaded modules
$ module purge
```

```bash
# Or, only unload miniconda
$ module unload miniconda3
```

#### Delete remaining Conda files
```bash
# When Conda is no longer used, delete the remaining Conda installation files/configuration files (please use it carefully!! Make sure there is no need for files in this path before using this command)
# Check the contents of this path file first (usually only ~/.conda)
$ ls ~/.condarc ~/.conda ~/.continuum

# Then, delete the files
$ rm -rf ~/.condarc ~/.conda ~/.continuum
```

### Step 2. Slurm job script with Conda

Once you have prepared the Conda environment and computing program, please follow the steps to write a job script using Slurm Workload Manager to request resources, queue schedules, and submit jobs according to your needs.


The `.sh` file is the job script submitted by Slurm, the content is divided into two parts:

1. **Information of Job, project and resources**: job name, Number of nodes, Number of jobs running per node, Number of GPUs per node, longest job running time, project ID, and queue name.
3. **The content of the job**


Write the following content and use the virtual environment `mytf_gpu` created by the above steps.


Run the following edit command > Press <kbd>i</kbd> key > Copy and paste the example > Press <kbd>ESC</kbd> key > Enter `:wq!` to save and exit. We're done!


```bash
$ vim <FILE_NAME>.sh
```

:::info 
<i class="fa fa-paperclip fa-20" aria-hidden="true"></i> **Note:** You may write `.sh`  files using the editor you're used to, the example uses vim to operate.
:::

- TensorFlow 2 with 1 node / 8GPUs
```bash
#!/bin/bash
#SBATCH --job-name=Hello_TWCC    ## job name
#SBATCH --nodes=1                ## request 1 node
#SBATCH --cpus-per-task=32       ## request 32 CPUs for that task
#SBATCH --gres=gpu:8             ## request 8 GPUs per node
#SBATCH --time=00:10:00          ## Run for up to 10 minutes (remember to change it after testing, or delete this line directly)
#SBATCH --account=PROJECT_ID     ## PROJECT_ID (e.g.,  MST108XXX). Billing will also be based on this project ID
#SBATCH --partition=gtest        ## gtest is used for testing queue. After subsequent tests, it can be changed to gp1d (maximum running 1 day), gp2d (maximum running 2 days), gp4d (maximum running 4 days)

module purge
module load miniconda3
conda activate mytf_gpu

# The programs of most of conda users do not communicate through MPI
# You do not need to add srun/mpirun, just add the commands that need to be run

python $(your command) 
```


:::info
<i class="fa fa-paperclip fa-20" aria-hidden="true"></i> **Note:** 
1. Adding the following content to the script file header, you may add an email to get the job state notification:
    ```bash
    #SBATCH --mail-type=ALL
    #SBATCH --mail-user=$Your_email
    ```
2. The ratio of the requested resources will be based on the number of GPUs you assign, and will be allocated based on the ratio of 1 GPU: 4 CPU: 90 GB Memory. For example,
    > Request 1 GPU, 90 GB Memory will be automatically allocated
      Request 8 GPU, 720GB Memory will be automatically allocated
3. For more information about queue, please refer to [<ins>Usage instructions of queue and computing resources</ins>](https://www.twcc.ai/doc?page=hpc_cli#4-Queue-%E8%88%87%E8%A8%88%E7%AE%97%E8%B3%87%E6%BA%90%E4%BD%BF%E7%94%A8%E8%AA%AA%E6%98%8E).
:::

#### Submit jobs

```bash
$ sbatch <FILE_NAME>.sh
```

:::info
<i class="fa fa-paperclip fa-20" aria-hidden="true"></i> **Note:** After submitting, it will show the Job ID distributed by the system.
:::

#### View and cancel

- Once start computing, you can run the following command to view the log.

```bash
$ tail -f slurm_<JOB_ID>.out
```

:::info
<i class="fa fa-paperclip fa-20" aria-hidden="true"></i> **Note:**
For more commonly used commands, see [<ins>Slurm command</ins>](https://www.twcc.ai/doc?page=hpc_cli#6-Slurm%E6%8C%87%E4%BB%A4):
1. Use`squeue -u $USER` to view the running job.
2. Use `sacct -X`to check today's running job and state to make sure whether it is still running or has already finished.
:::


- For canceling the running job, run the command:

```bash
$ scancel <JOB_ID>
